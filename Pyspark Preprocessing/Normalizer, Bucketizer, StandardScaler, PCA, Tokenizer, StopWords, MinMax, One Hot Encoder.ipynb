{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import io, pandas as pd\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv('/home/pygmy/Projects/Veni/Bisnis_Pintar/Iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalizer = Normalizer(p=2.0, inputCol=\"PetalWidthCm\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+---+--------------+------------------+\n",
      "| id|      features|      normFeatures|\n",
      "+---+--------------+------------------+\n",
      "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
      "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
      "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
      "+---+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [(0.1,), (0.4,), (1.2,), (1.5,), (float(\"nan\"),), (float(\"nan\"),)]\n",
    "df = spark.createDataFrame(values, [\"values\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketizer = Bucketizer(splits=[-float(\"inf\"), 0.5, 1.4, float(\"inf\")],\n",
    "                        inputCol=\"values\", outputCol=\"buckets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketed = bucketizer.setHandleInvalid(\"keep\").transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|values|buckets|\n",
      "+------+-------+\n",
      "|   0.1|    0.0|\n",
      "|   0.4|    0.0|\n",
      "|   1.2|    1.0|\n",
      "|   1.5|    2.0|\n",
      "|   NaN|    3.0|\n",
      "|   NaN|    3.0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import PCA\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data,[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pca.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pca.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|        pca_features|\n",
      "+--------------------+--------------------+\n",
      "| (5,[1,3],[1.0,7.0])|[1.64857282308838...|\n",
      "|[2.0,0.0,3.0,4.0,...|[-4.6451043317815...|\n",
      "|[4.0,0.0,0.0,6.0,...|[-6.4288805356764...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standar Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(Vectors.dense([0.0]),), (Vectors.dense([2.0]),)], [\"a\"])\n",
    "model = StandardScaler(inputCol=\"a\", outputCol=\"scaled\")\n",
    "model = model.fit(df)\n",
    "model.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|    a|             scaled|\n",
      "+-----+-------------------+\n",
      "|[0.0]|              [0.0]|\n",
      "|[2.0]|[1.414213562373095]|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|               items|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Raw Shrimp, Seedl...|[raw, shrimp,, se...|\n",
      "|  1|Cracked Wheat, St...|[cracked, wheat,,...|\n",
      "|  2|Beet Apple Carrot...|[beet, apple, car...|\n",
      "|  3|               Vodka|             [vodka]|\n",
      "|  4|Globe Eggplant, P...|[globe, eggplant,...|\n",
      "|  5|Organic Baby Spin...|[organic, baby, s...|\n",
      "|  6|Reduced Fat Crack...|[reduced, fat, cr...|\n",
      "|  7|Organic Red Onion...|[organic, red, on...|\n",
      "|  8|Organic Cripps Pi...|[organic, cripps,...|\n",
      "|  9|Organic Baby Spin...|[organic, baby, s...|\n",
      "| 10|Uncured Beef Hot ...|[uncured, beef, h...|\n",
      "| 11|Donut House Choco...|[donut, house, ch...|\n",
      "| 12|[Concentrated But...|[[concentrated, b...|\n",
      "| 13|Raspberries, Gree...|[raspberries,, gr...|\n",
      "| 14|Original Tofurky ...|[original, tofurk...|\n",
      "| 15|Extra Hold Non-Ae...|[extra, hold, non...|\n",
      "| 16|Organic Coconut M...|[organic, coconut...|\n",
      "| 17|No. 485 Gin, Mont...|[no., 485, gin,, ...|\n",
      "| 18|Red Vine Tomato, ...|[red, vine, tomat...|\n",
      "| 19|Organic Baby Arug...|[organic, baby, a...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/pygmy/Projects/Moti/public.items.csv\", sep=\"|\")\n",
    "df = spark.createDataFrame(df)\n",
    "tokenizer = Tokenizer(inputCol=\"items\", outputCol=\"words\")\n",
    "tokenizer = tokenizer.transform(df)\n",
    "tokenizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|               items|               words|             removed|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|Raw Shrimp, Seedl...|[raw, shrimp,, se...|[raw, shrimp,, se...|\n",
      "|  1|Cracked Wheat, St...|[cracked, wheat,,...|[cracked, wheat,,...|\n",
      "|  2|Beet Apple Carrot...|[beet, apple, car...|[beet, apple, car...|\n",
      "|  3|               Vodka|             [vodka]|             [vodka]|\n",
      "|  4|Globe Eggplant, P...|[globe, eggplant,...|[globe, eggplant,...|\n",
      "|  5|Organic Baby Spin...|[organic, baby, s...|[organic, baby, s...|\n",
      "|  6|Reduced Fat Crack...|[reduced, fat, cr...|[reduced, fat, cr...|\n",
      "|  7|Organic Red Onion...|[organic, red, on...|[organic, red, on...|\n",
      "|  8|Organic Cripps Pi...|[organic, cripps,...|[organic, cripps,...|\n",
      "|  9|Organic Baby Spin...|[organic, baby, s...|[organic, baby, s...|\n",
      "| 10|Uncured Beef Hot ...|[uncured, beef, h...|[uncured, beef, h...|\n",
      "| 11|Donut House Choco...|[donut, house, ch...|[donut, house, ch...|\n",
      "| 12|[Concentrated But...|[[concentrated, b...|[[concentrated, b...|\n",
      "| 13|Raspberries, Gree...|[raspberries,, gr...|[raspberries,, gr...|\n",
      "| 14|Original Tofurky ...|[original, tofurk...|[original, tofurk...|\n",
      "| 15|Extra Hold Non-Ae...|[extra, hold, non...|[extra, hold, non...|\n",
      "| 16|Organic Coconut M...|[organic, coconut...|[organic, coconut...|\n",
      "| 17|No. 485 Gin, Mont...|[no., 485, gin,, ...|[no., 485, gin,, ...|\n",
      "| 18|Red Vine Tomato, ...|[red, vine, tomat...|[red, vine, tomat...|\n",
      "| 19|Organic Baby Arug...|[organic, baby, a...|[organic, baby, a...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"removed\")\n",
    "remover.transform(tokenizer).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, items: string, words: array<string>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|                 raw|            filtered|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[I, saw, the, red...| [saw, red, balloon]|\n",
      "|  1|[Mary, had, a, li...|[Mary, little, lamb]|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover.transform(sentenceData).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec|\n",
      "+---+--------+-------------+-------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|\n",
      "|  1|       b|          2.0|    (2,[],[])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|\n",
      "+---+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/pygmy/Projects/Moti/public.items.csv\", sep=\"|\")\n",
    "df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               items|\n",
      "+---+--------------------+\n",
      "|  0|Raw Shrimp, Seedl...|\n",
      "|  1|Cracked Wheat, St...|\n",
      "|  2|Beet Apple Carrot...|\n",
      "|  3|               Vodka|\n",
      "|  4|Globe Eggplant, P...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"items\", outputCol=\"categoryIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------+\n",
      "| id|               items|categoryIndex|\n",
      "+---+--------------------+-------------+\n",
      "|  0|Raw Shrimp, Seedl...|       3304.0|\n",
      "|  1|Cracked Wheat, St...|      12898.0|\n",
      "|  2|Beet Apple Carrot...|       1166.0|\n",
      "|  3|               Vodka|        103.0|\n",
      "|  4|Globe Eggplant, P...|     116559.0|\n",
      "|  5|Organic Baby Spin...|     100966.0|\n",
      "|  6|Reduced Fat Crack...|      86958.0|\n",
      "|  7|Organic Red Onion...|      79118.0|\n",
      "|  8|Organic Cripps Pi...|      69377.0|\n",
      "|  9|Organic Baby Spin...|     104739.0|\n",
      "| 10|Uncured Beef Hot ...|     106295.0|\n",
      "| 11|Donut House Choco...|       3993.0|\n",
      "| 12|[Concentrated But...|      39701.0|\n",
      "| 13|Raspberries, Gree...|     114490.0|\n",
      "| 14|Original Tofurky ...|      15893.0|\n",
      "| 15|Extra Hold Non-Ae...|      58880.0|\n",
      "| 16|Organic Coconut M...|      69949.0|\n",
      "| 17|No. 485 Gin, Mont...|      37464.0|\n",
      "| 18|Red Vine Tomato, ...|      93873.0|\n",
      "| 19|Organic Baby Arug...|      84761.0|\n",
      "+---+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------+--------------------+\n",
      "| id|               items|categoryIndex|         categoryVec|\n",
      "+---+--------------------+-------------+--------------------+\n",
      "|  0|Raw Shrimp, Seedl...|       3304.0|(126765,[3304],[1...|\n",
      "|  1|Cracked Wheat, St...|      12898.0|(126765,[12898],[...|\n",
      "|  2|Beet Apple Carrot...|       1166.0|(126765,[1166],[1...|\n",
      "|  3|               Vodka|        103.0|(126765,[103],[1.0])|\n",
      "|  4|Globe Eggplant, P...|     116559.0|(126765,[116559],...|\n",
      "|  5|Organic Baby Spin...|     100966.0|(126765,[100966],...|\n",
      "|  6|Reduced Fat Crack...|      86958.0|(126765,[86958],[...|\n",
      "|  7|Organic Red Onion...|      79118.0|(126765,[79118],[...|\n",
      "|  8|Organic Cripps Pi...|      69377.0|(126765,[69377],[...|\n",
      "|  9|Organic Baby Spin...|     104739.0|(126765,[104739],...|\n",
      "| 10|Uncured Beef Hot ...|     106295.0|(126765,[106295],...|\n",
      "| 11|Donut House Choco...|       3993.0|(126765,[3993],[1...|\n",
      "| 12|[Concentrated But...|      39701.0|(126765,[39701],[...|\n",
      "| 13|Raspberries, Gree...|     114490.0|(126765,[114490],...|\n",
      "| 14|Original Tofurky ...|      15893.0|(126765,[15893],[...|\n",
      "| 15|Extra Hold Non-Ae...|      58880.0|(126765,[58880],[...|\n",
      "| 16|Organic Coconut M...|      69949.0|(126765,[69949],[...|\n",
      "| 17|No. 485 Gin, Mont...|      37464.0|(126765,[37464],[...|\n",
      "| 18|Red Vine Tomato, ...|      93873.0|(126765,[93873],[...|\n",
      "| 19|Organic Baby Arug...|      84761.0|(126765,[84761],[...|\n",
      "+---+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(Vectors.dense([0.0]),), (Vectors.dense([2.0]),)], [\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmScaler = MinMaxScaler(inputCol=\"a\", outputCol=\"scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mmScaler.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|    a|scaled|\n",
      "+-----+------+\n",
      "|[0.0]| [0.0]|\n",
      "|[2.0]| [1.0]|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec|\n",
      "+---+--------+-------------+-------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|\n",
      "|  1|       b|          2.0|    (2,[],[])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|\n",
      "+---+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\", handleInvalid=\"error\",\n",
    "                              stringOrderType=\"frequencyDesc\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
